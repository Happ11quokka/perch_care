"""
knowledge ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²­í‚¹í•˜ì—¬ ChromaDBì— ì ì¬í•˜ëŠ” ëª¨ë“ˆ.
"""

import os
import time
from pathlib import Path

import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

from chunker import parse_markdown_into_chunks, discover_knowledge_files

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ (agent/ ì˜ ìƒìœ„)
PROJECT_ROOT = Path(__file__).parent.parent
CHROMA_DATA_DIR = Path(__file__).parent / "chroma_data"
COLLECTION_NAME = "parrot_knowledge"

KNOWLEDGE_DIRS = [
    {"path": PROJECT_ROOT / "knowledge", "language": "en"},
    {"path": PROJECT_ROOT / "knowledge-zh", "language": "zh"},
]


def get_embedding_function() -> OpenAIEmbeddingFunction:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return OpenAIEmbeddingFunction(
        api_key=api_key,
        model_name="text-embedding-3-large",
    )


def get_chroma_client() -> chromadb.PersistentClient:
    return chromadb.PersistentClient(path=str(CHROMA_DATA_DIR))


def get_collection(client: chromadb.PersistentClient, embedding_fn=None):
    if embedding_fn is None:
        embedding_fn = get_embedding_function()
    return client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_fn,
        metadata={"hnsw:space": "cosine"},
    )


def load_knowledge(reset: bool = False) -> dict:
    """knowledge ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²­í‚¹í•˜ì—¬ ChromaDBì— ì ì¬."""
    from dotenv import load_dotenv
    load_dotenv(Path(__file__).parent / ".env")

    client = get_chroma_client()
    embedding_fn = get_embedding_function()

    if reset:
        try:
            client.delete_collection(COLLECTION_NAME)
            print("ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ.")
        except ValueError:
            pass

    collection = get_collection(client, embedding_fn)

    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    existing_count = collection.count()
    if existing_count > 0 and not reset:
        print(f"ì´ë¯¸ {existing_count}ê°œ ì²­í¬ê°€ ì ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. --reset ì˜µì…˜ìœ¼ë¡œ ì¬ì ì¬í•˜ì„¸ìš”.")
        return {"status": "already_loaded", "count": existing_count}

    all_chunks = []
    file_count = 0
    stats = {"by_category": {}, "by_language": {"en": 0, "zh": 0}}

    for kdir in KNOWLEDGE_DIRS:
        knowledge_path = kdir["path"]
        language = kdir["language"]

        if not knowledge_path.exists():
            print(f"ê²½ê³ : {knowledge_path} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µ.")
            continue

        files = discover_knowledge_files(knowledge_path)
        print(f"\nğŸ“‚ {knowledge_path.name}/ â€” {len(files)}ê°œ íŒŒì¼ ë°œê²¬ (language={language})")

        for file_info in files:
            content = file_info["path"].read_text(encoding="utf-8")
            chunks = parse_markdown_into_chunks(
                content=content,
                source=file_info["source"],
                category=file_info["category"],
                language=language,
            )

            for chunk in chunks:
                all_chunks.append(chunk)
                cat = chunk["category"]
                stats["by_category"][cat] = stats["by_category"].get(cat, 0) + 1
                stats["by_language"][language] += 1

            file_count += 1
            if file_count % 50 == 0:
                print(f"  ... {file_count}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")

    if not all_chunks:
        print("ì ì¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"status": "empty", "count": 0}

    # ChromaDBì— ë°°ì¹˜ ì‚½ì… (ChromaDBê°€ ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„±)
    print(f"\nì´ {len(all_chunks)}ê°œ ì²­í¬ë¥¼ ChromaDBì— ì ì¬ ì¤‘...")
    start_time = time.time()

    batch_size = 100
    for i in range(0, len(all_chunks), batch_size):
        batch = all_chunks[i : i + batch_size]
        ids = [f"chunk_{i + j}" for j in range(len(batch))]
        documents = [c["content"] for c in batch]
        metadatas = [
            {
                "source": c["source"],
                "category": c["category"],
                "language": c["language"],
                "section_title": c["section_title"] or "",
            }
            for c in batch
        ]
        collection.add(ids=ids, documents=documents, metadatas=metadatas)
        print(f"  ... {min(i + batch_size, len(all_chunks))}/{len(all_chunks)} ì ì¬ ì™„ë£Œ")

    elapsed = time.time() - start_time
    print(f"\nì ì¬ ì™„ë£Œ! ({elapsed:.1f}ì´ˆ)")
    print(f"  íŒŒì¼: {file_count}ê°œ")
    print(f"  ì²­í¬: {len(all_chunks)}ê°œ")
    print(f"\nì¹´í…Œê³ ë¦¬ë³„:")
    for cat, count in sorted(stats["by_category"].items()):
        print(f"  {cat}: {count}ê°œ")
    print(f"\nì–¸ì–´ë³„:")
    for lang, count in stats["by_language"].items():
        print(f"  {lang}: {count}ê°œ")

    return {
        "status": "loaded",
        "count": len(all_chunks),
        "files": file_count,
        "stats": stats,
        "elapsed_seconds": elapsed,
    }


if __name__ == "__main__":
    import sys
    reset = "--reset" in sys.argv
    load_knowledge(reset=reset)
