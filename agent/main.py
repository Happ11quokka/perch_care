"""
ì•µë°•ì‚¬ AI ì—ì´ì „íŠ¸ â€” FastAPI ì•± + CLI ì§„ì…ì .

ì‚¬ìš©ë²•:
  python main.py                            # FastAPI ì„œë²„ ì‹¤í–‰ (localhost:8100)
  python main.py --load                     # ì§€ì‹ ë°ì´í„° ì ì¬
  python main.py --load --reset             # ì´ˆê¸°í™” í›„ ì¬ì ì¬
  python main.py --stats                    # ì ì¬ëœ ë°ì´í„° í†µê³„
  python main.py --search "feather plucking" # ë²¡í„° ê²€ìƒ‰ë§Œ
  python main.py --ask "Can parrots eat avocado?" # ë‹¨ì¼ ì§ˆë¬¸
  python main.py --interactive              # ì¸í„°ë™í‹°ë¸Œ ì±„íŒ…
  python main.py --benchmark                # ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
"""

import argparse
import os
import sys
import time
from pathlib import Path

from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv(Path(__file__).parent / ".env")

# --- FastAPI ì•± ---
from fastapi import FastAPI, Query
from pydantic import BaseModel, Field

app = FastAPI(
    title="ì•µë°•ì‚¬ AI Agent",
    description="ì•µë¬´ìƒˆ ì§€ì‹ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì—ì´ì „íŠ¸",
    version="0.1.0",
)


class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=2000)
    top_k: int = Field(5, ge=1, le=20)
    category: str | None = None
    language: str | None = None


class AskRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=2000)
    history: list[dict] = Field(default_factory=list)
    model: str = "gpt-4o-mini"
    max_tokens: int = 1024
    top_k: int = 5


@app.post("/search")
def api_search(req: SearchRequest):
    """ë²¡í„° ê²€ìƒ‰ë§Œ ì‹¤í–‰, ìœ ì‚¬ë„ ê²°ê³¼ ë°˜í™˜."""
    import vector_store

    start = time.time()
    results = vector_store.search(
        query=req.query,
        top_k=req.top_k,
        category=req.category,
        language=req.language,
    )
    elapsed = time.time() - start
    return {
        "query": req.query,
        "results": results,
        "count": len(results),
        "elapsed_ms": round(elapsed * 1000, 1),
    }


@app.post("/ask")
def api_ask(req: AskRequest):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ â†’ GPT ë‹µë³€ + ì†ŒìŠ¤."""
    import chat as chat_module

    start = time.time()
    answer, sources = chat_module.chat(
        query=req.query,
        history=req.history,
        model=req.model,
        max_tokens=req.max_tokens,
        top_k=req.top_k,
    )
    elapsed = time.time() - start
    return {
        "query": req.query,
        "answer": answer,
        "sources": [
            {"source": s["source"], "section": s["section_title"], "similarity": s["similarity"]}
            for s in sources
        ],
        "elapsed_ms": round(elapsed * 1000, 1),
    }


@app.get("/stats")
def api_stats():
    """ì ì¬ëœ ë°ì´í„° í†µê³„."""
    import vector_store

    return vector_store.get_stats()


@app.post("/load")
def api_load(reset: bool = Query(False)):
    """ì§€ì‹ ë°ì´í„° ì ì¬."""
    from knowledge_loader import load_knowledge

    return load_knowledge(reset=reset)


# --- CLI ëª¨ë“œ ---

BENCHMARK_QUERIES = [
    {"query": "My parrot is plucking its feathers", "lang": "EN", "expected": "feather plucking, FDB"},
    {"query": "Can parrots eat avocado?", "lang": "EN", "expected": "avocado, persin, toxic"},
    {"query": "What are symptoms of psittacosis?", "lang": "EN", "expected": "psittacosis, chlamydia"},
    {"query": "ì•µë¬´ìƒˆê°€ ê¹ƒí„¸ì„ ë½‘ì•„ìš”", "lang": "KO", "expected": "feather plucking (cross-lingual)"},
    {"query": "ì•µë¬´ìƒˆì—ê²Œ ì•„ë³´ì¹´ë„ë¥¼ ì¤˜ë„ ë˜ë‚˜ìš”?", "lang": "KO", "expected": "avocado, toxic"},
    {"query": "æˆ‘çš„é¹¦é¹‰æ‹”è‡ªå·±çš„ç¾½æ¯›æ€ä¹ˆåŠ", "lang": "ZH", "expected": "å•„ç¾½, feather plucking"},
    {"query": "é¹¦é¹‰å¯ä»¥åƒç‰›æ²¹æœå—", "lang": "ZH", "expected": "avocado, toxic"},
    {"query": "è™çš®é¹¦é¹‰æ€ä¹ˆè®­ç»ƒä¸Šæ‰‹", "lang": "ZH", "expected": "ä¸Šæ‰‹è®­ç»ƒ, step-up"},
]


def cli_search(query: str):
    """CLI: ë²¡í„° ê²€ìƒ‰."""
    import vector_store

    print(f"\nğŸ” ê²€ìƒ‰: \"{query}\"\n")
    results = vector_store.search(query, top_k=5)
    if not results:
        print("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")
        return
    for i, r in enumerate(results, 1):
        print(f"  [{i}] similarity={r['similarity']:.4f}  |  {r['source']}  |  {r['section_title']}")
        # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²« 200ì)
        preview = r["content"][:200].replace("\n", " ")
        print(f"      {preview}...")
        print()


def cli_ask(query: str):
    """CLI: ë‹¨ì¼ ì§ˆë¬¸."""
    import chat as chat_module

    print(f"\nğŸ’¬ ì§ˆë¬¸: \"{query}\"\n")
    start = time.time()
    answer, sources = chat_module.chat(query)
    elapsed = time.time() - start

    print(f"ğŸ¦œ ì•µë°•ì‚¬:\n{answer}\n")
    if sources:
        print(f"ğŸ“š ì°¸ì¡° ì†ŒìŠ¤ ({len(sources)}ê±´):")
        for s in sources:
            print(f"  - {s['source']} ({s['section_title']}) [similarity: {s['similarity']:.2f}]")
    print(f"\nâ±  {elapsed:.1f}ì´ˆ")


def cli_interactive():
    """CLI: ì¸í„°ë™í‹°ë¸Œ ì±„íŒ…."""
    import chat as chat_module

    print("\nğŸ¦œ ì•µë°•ì‚¬ AI ì—ì´ì „íŠ¸ (ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ)")
    print("   ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.\n")

    history = []
    while True:
        try:
            query = input("> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not query:
            continue
        if query.lower() in ("quit", "exit", "q"):
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        start = time.time()
        answer, sources = chat_module.chat(query, history=history)
        elapsed = time.time() - start

        print(f"\nğŸ¦œ ì•µë°•ì‚¬:\n{answer}\n")
        if sources:
            print(f"ğŸ“š ì°¸ì¡° ({len(sources)}ê±´):", end=" ")
            for s in sources[:3]:
                print(f"[{s['source']}]", end=" ")
            print()
        print(f"â±  {elapsed:.1f}ì´ˆ\n")

        history.append({"role": "user", "content": query})
        history.append({"role": "assistant", "content": answer})
        # íˆìŠ¤í† ë¦¬ ìµœëŒ€ 10ê°œ ìœ ì§€
        if len(history) > 20:
            history = history[-20:]


def cli_benchmark():
    """CLI: ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸."""
    import vector_store
    import chat as chat_module

    print("\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    print("=" * 80)

    for i, bq in enumerate(BENCHMARK_QUERIES, 1):
        query = bq["query"]
        lang = bq["lang"]
        expected = bq["expected"]

        print(f"\n[{i}/{len(BENCHMARK_QUERIES)}] ({lang}) \"{query}\"")
        print(f"  ê¸°ëŒ€: {expected}")

        # ê²€ìƒ‰ ê²°ê³¼
        results = vector_store.search(query, top_k=3)
        if results:
            print(f"  ê²€ìƒ‰ ê²°ê³¼:")
            for j, r in enumerate(results, 1):
                print(f"    [{j}] sim={r['similarity']:.3f} | {r['source']} | {r['section_title']}")
        else:
            print(f"  ê²€ìƒ‰ ê²°ê³¼: ì—†ìŒ âŒ")

        # GPT ë‹µë³€
        start = time.time()
        answer, _ = chat_module.chat(query, top_k=3)
        elapsed = time.time() - start
        preview = answer[:150].replace("\n", " ")
        print(f"  ë‹µë³€ ({elapsed:.1f}s): {preview}...")
        print("-" * 80)

    print("\në²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")


def cli_stats():
    """CLI: í†µê³„ ì¶œë ¥."""
    import vector_store

    stats = vector_store.get_stats()
    print(f"\nğŸ“Š Knowledge Base í†µê³„")
    print(f"  ì´ ì²­í¬: {stats['total']}ê°œ\n")

    if stats["by_category"]:
        print("  ì¹´í…Œê³ ë¦¬ë³„:")
        for cat, count in stats["by_category"].items():
            print(f"    {cat}: {count}ê°œ")

    if stats["by_language"]:
        print("\n  ì–¸ì–´ë³„:")
        for lang, count in stats["by_language"].items():
            print(f"    {lang}: {count}ê°œ")
    print()


def main():
    parser = argparse.ArgumentParser(description="ì•µë°•ì‚¬ AI ì—ì´ì „íŠ¸")
    parser.add_argument("--load", action="store_true", help="ì§€ì‹ ë°ì´í„° ì ì¬")
    parser.add_argument("--reset", action="store_true", help="ì ì¬ ì‹œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ")
    parser.add_argument("--stats", action="store_true", help="ì ì¬ëœ ë°ì´í„° í†µê³„")
    parser.add_argument("--search", type=str, help="ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰")
    parser.add_argument("--ask", type=str, help="ë‹¨ì¼ ì§ˆë¬¸")
    parser.add_argument("--interactive", action="store_true", help="ì¸í„°ë™í‹°ë¸Œ ì±„íŒ…")
    parser.add_argument("--benchmark", action="store_true", help="ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8100, help="FastAPI í¬íŠ¸ (ê¸°ë³¸: 8100)")

    args = parser.parse_args()

    if args.load:
        from knowledge_loader import load_knowledge
        load_knowledge(reset=args.reset)
    elif args.stats:
        cli_stats()
    elif args.search:
        cli_search(args.search)
    elif args.ask:
        cli_ask(args.ask)
    elif args.interactive:
        cli_interactive()
    elif args.benchmark:
        cli_benchmark()
    else:
        # FastAPI ì„œë²„ ì‹¤í–‰
        import uvicorn
        print(f"\nğŸ¦œ ì•µë°•ì‚¬ AI ì—ì´ì „íŠ¸ ì„œë²„ ì‹œì‘")
        print(f"   http://localhost:{args.port}")
        print(f"   Swagger UI: http://localhost:{args.port}/docs\n")
        uvicorn.run(app, host="0.0.0.0", port=args.port)


if __name__ == "__main__":
    main()
